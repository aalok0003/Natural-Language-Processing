{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import requests\\nfrom bs4 import BeautifulSoup\\nurl='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\\n\\n\\n# Make a GET request to the website\\nresponse = requests.get(url)\\n\\n# Parse the HTML content of the website using Beautiful Soup\\nsoup = BeautifulSoup(response.content, 'html.parser')\\n\\n# Find the main content area of the website\\nmain_content = soup.find('main')\\n\\n# Exclude any header or footer content\\nheader_content = main_content.find('header')\\nif header_content:\\n    header_content.decompose()\\n    \\nfooter_content = main_content.find('footer')\\nif footer_content:\\n    footer_content.decompose()\\n\\n# Extract the text content from the main content area\\ntext_content = main_content.get_text()\\n\\n# Print the text content\\nprint(text_content)\\n\\n \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the website using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the main content area of the website\n",
    "main_content = soup.find('main')\n",
    "\n",
    "# Exclude any header or footer content\n",
    "header_content = main_content.find('header')\n",
    "if header_content:\n",
    "    header_content.decompose()\n",
    "    \n",
    "footer_content = main_content.find('footer')\n",
    "if footer_content:\n",
    "    footer_content.decompose()\n",
    "\n",
    "# Extract the text content from the main content area\n",
    "text_content = main_content.get_text()\n",
    "\n",
    "# Print the text content\n",
    "print(text_content)\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import requests\\nfrom bs4 import BeautifulSoup\\nurl='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\\n\\n\\n# Make a GET request to the website\\nresponse = requests.get(url)\\n\\n# Parse the HTML content of the website using Beautiful Soup\\nsoup = BeautifulSoup(response.content, 'html.parser')\\n\\n# Find all div elements with the class example-class\\nexample_divs = soup.find_all('div', {'class':'tdb-block-inner td-fix-index'})\\n\\n# Extract text data from each div element\\nfor div in example_divs:\\n    text_data = div.get_text()\\n    print(text_data)\\n\\n    file_name = url.split('/')[-1] + '37 AI in healthcare to Improve Patient Outcome.txt'\\n    with open(file_name, 'w') as f:\\n        f.write(text_data)\\n \\n\\n\\n\\n\\n \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the website using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all div elements with the class example-class\n",
    "example_divs = soup.find_all('div', {'class':'tdb-block-inner td-fix-index'})\n",
    "\n",
    "# Extract text data from each div element\n",
    "for div in example_divs:\n",
    "    text_data = div.get_text()\n",
    "    print(text_data)\n",
    "\n",
    "    file_name = url.split('/')[-1] + '37 AI in healthcare to Improve Patient Outcome.txt'\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(text_data)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import requests\\nfrom bs4 import BeautifulSoup\\n\\n# Set the URL of the web page to scrape\\nurl='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\\n\\n\\n# Make a GET request to the website\\nresponse = requests.get(url)\\n\\n# Parse the HTML content of the website using Beautiful Soup\\nsoup = BeautifulSoup(response.content, 'html.parser')\\n\\n# Find the div element with the class 'article-text'\\narticle_div = soup.find('div', {'class':'tdb-block-inner td-fix-index'})\\n\\n# Extract the text content from the div element\\ntext_content = article_div.get_text()\\n\\n# Set the file name to the URL ID\\nfile_name = url.split('/')[-1] + '37 AI in healthcare to Improve Patient Outcome.txt'\\n\\n# Save the extracted article in a text file\\nwith open(file_name, 'w') as f:\\n    f.write(text_content)\\n\\n# Print the file name to the console\\nprint('Article saved as:', file_name)\\n\\n \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the URL of the web page to scrape\n",
    "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the website using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the div element with the class 'article-text'\n",
    "article_div = soup.find('div', {'class':'tdb-block-inner td-fix-index'})\n",
    "\n",
    "# Extract the text content from the div element\n",
    "text_content = article_div.get_text()\n",
    "\n",
    "# Set the file name to the URL ID\n",
    "file_name = url.split('/')[-1] + '37 AI in healthcare to Improve Patient Outcome.txt'\n",
    "\n",
    "# Save the extracted article in a text file\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write(text_content)\n",
    "\n",
    "# Print the file name to the console\n",
    "print('Article saved as:', file_name)\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article saved as: ai-in-healthcare-to-improve-patient-outcomes.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the URL of the web page to scrape\n",
    "url = 'https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the website using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all div elements with the class 'tdb-block-inner td-fix-index'\n",
    "example_divs = soup.find_all('div', {'class': 'tdb-block-inner td-fix-index'})\n",
    "\n",
    "# Extract text data from each div element and save it to a single file\n",
    "text_data = ''\n",
    "for div in example_divs:\n",
    "    text_data += div.get_text() + '\\n'\n",
    "\n",
    "# Set the file name to the URL ID and save the extracted article in a text file\n",
    "file_name = url.split('/')[-2] + '.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write(text_data)\n",
    "\n",
    "# Print the file name to the console\n",
    "print('Article saved as:', file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 2074\n",
      "Sentiment: {'neg': 0.043, 'neu': 0.812, 'pos': 0.145, 'compound': 0.9997}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Tokenize the text content into words\n",
    "words = nltk.word_tokenize(text_data)\n",
    "\n",
    "# Calculate the word count\n",
    "word_count = len(words)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment = sid.polarity_scores(text_data)\n",
    "\n",
    "# Print the results to the console\n",
    "print('Word count:', word_count)\n",
    "print('Sentiment:', sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 2074\n",
      "Sentiment positive: 0.145\n",
      "Sentiment negative: 0.043\n",
      "Sentiment neutral: 0.812\n",
      "PCW: 18.23%\n",
      "Fog Index: 14.65\n",
      "Personal pronoun count: Counter({'it': 16, 'they': 4, 'our': 3, 'their': 3, 'its': 2, 'we': 1, 'us': 1, 'them': 1})\n",
      "Personal pronoun percentage: 1.49%\n",
      "syllable_per_word: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\alok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import textstat\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Tokenize the text content into words\n",
    "words = nltk.word_tokenize(text_data)\n",
    "\n",
    "# Calculate the word count\n",
    "word_count = len(words)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = sid.polarity_scores(text_data)\n",
    "\n",
    "# Extract the sentiment scores\n",
    "sentiment_positive = sentiment_scores['pos']\n",
    "sentiment_negative = sentiment_scores['neg']\n",
    "sentiment_neutral = sentiment_scores['neu']\n",
    "\n",
    "# Print the results to the console\n",
    "print('Word count:', word_count)\n",
    "print('Sentiment positive:', sentiment_positive)\n",
    "print('Sentiment negative:', sentiment_negative)\n",
    "print('Sentiment neutral:', sentiment_neutral)\n",
    "\n",
    "complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n",
    "\n",
    "# Calculate the total number of words\n",
    "total_word_count = len(words)\n",
    "\n",
    "# Calculate the PCW as the percentage of complex words in the text\n",
    "pcw = complex_word_count / total_word_count * 100\n",
    "\n",
    "# Print the PCW to the console\n",
    "print(f\"PCW: {pcw:.2f}%\")\n",
    "\n",
    "# Calculate the Fog Index\n",
    "fog_index = textstat.gunning_fog(text_data)\n",
    "\n",
    "# Print the Fog Index to the console\n",
    "print(\"Fog Index:\", fog_index)\n",
    "\n",
    "\n",
    "\n",
    "# Define the set of personal pronouns\n",
    "personal_pronouns = set(['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs'])\n",
    "\n",
    "# Count the frequency of personal pronouns in the text\n",
    "personal_pronoun_count = Counter(word.lower() for word in words if word.lower() in personal_pronouns)\n",
    "\n",
    "# Calculate the total number of words\n",
    "total_word_count = len(words)\n",
    "\n",
    "# Calculate the percentage of personal pronouns in the text\n",
    "personal_pronoun_percentage = sum(personal_pronoun_count.values()) / total_word_count * 100\n",
    "\n",
    "# Print the results to the console\n",
    "print(f\"Personal pronoun count: {personal_pronoun_count}\")\n",
    "print(f\"Personal pronoun percentage: {personal_pronoun_percentage:.2f}%\")\n",
    "\n",
    "# Calculate the total number of syllables\n",
    "total_syllables = sum(textstat.syllable_count(word) for word in words)\n",
    "\n",
    "# Calculate the total number of words\n",
    "total_words = len(words)\n",
    "\n",
    "# Calculate the SPW as the average number of syllables per word\n",
    "syllable_per_word = total_syllables / total_words\n",
    "\n",
    "# Print the SPW to the console\n",
    "print(f\"syllable_per_word: {syllable_per_word:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity score: 0.4589452762999276\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Create a TextBlob object with the text content\n",
    "blob = TextBlob(text_data)\n",
    "\n",
    "# Calculate the subjectivity score\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "# Print the subjectivity score to the console\n",
    "print('Subjectivity score:', subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 26.935064935064936\n",
      "Average number of words per sentence: 26.94\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text_data)\n",
    "\n",
    "# Calculate the total number of words and sentences\n",
    "num_words = 0\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Increment the word count\n",
    "    num_words += len(words)\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sentence_length = num_words / num_sentences\n",
    "\n",
    "# Print the result to the console\n",
    "print('Average sentence length:', avg_sentence_length)\n",
    "\n",
    "# Calculate the average number of words per sentence\n",
    "avg_word_count = total_word_count / num_sentences\n",
    "\n",
    "# Print the result to the console\n",
    "print(f\"Average number of words per sentence: {avg_word_count:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
