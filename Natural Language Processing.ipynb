{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import requests\\nfrom bs4 import BeautifulSoup\\nurl='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\\n\\n\\n# Make a GET request to the website\\nresponse = requests.get(url)\\n\\n# Parse the HTML content of the website using Beautiful Soup\\nsoup = BeautifulSoup(response.content, 'html.parser')\\n\\n# Find the main content area of the website\\nmain_content = soup.find('main')\\n\\n# Exclude any header or footer content\\nheader_content = main_content.find('header')\\nif header_content:\\n    header_content.decompose()\\n    \\nfooter_content = main_content.find('footer')\\nif footer_content:\\n    footer_content.decompose()\\n\\n# Extract the text content from the main content area\\ntext_content = main_content.get_text()\\n\\n# Print the text content\\nprint(text_content)\\n\\n \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the website using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the main content area of the website\n",
    "main_content = soup.find('main')\n",
    "\n",
    "# Exclude any header or footer content\n",
    "header_content = main_content.find('header')\n",
    "if header_content:\n",
    "    header_content.decompose()\n",
    "    \n",
    "footer_content = main_content.find('footer')\n",
    "if footer_content:\n",
    "    footer_content.decompose()\n",
    "\n",
    "# Extract the text content from the main content area\n",
    "text_content = main_content.get_text()\n",
    "\n",
    "# Print the text content\n",
    "print(text_content)\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" import requests\\nfrom bs4 import BeautifulSoup\\nurl='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\\n\\n\\n# Make a GET request to the website\\nresponse = requests.get(url)\\n\\n# Parse the HTML content of the website using Beautiful Soup\\nsoup = BeautifulSoup(response.content, 'html.parser')\\n\\n# Find all div elements with the class example-class\\nexample_divs = soup.find_all('div', {'class':'tdb-block-inner td-fix-index'})\\n\\n# Extract text data from each div element\\nfor div in example_divs:\\n    text_data = div.get_text()\\n    print(text_data)\\n\\n    file_name = url.split('/')[-1] + '37 AI in healthcare to Improve Patient Outcome.txt'\\n    with open(file_name, 'w') as f:\\n        f.write(text_data)\\n \\n\\n\\n\\n\\n \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the website using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all div elements with the class example-class\n",
    "example_divs = soup.find_all('div', {'class':'tdb-block-inner td-fix-index'})\n",
    "\n",
    "# Extract text data from each div element\n",
    "for div in example_divs:\n",
    "    text_data = div.get_text()\n",
    "    print(text_data)\n",
    "\n",
    "    file_name = url.split('/')[-1] + '37 AI in healthcare to Improve Patient Outcome.txt'\n",
    "    with open(file_name, 'w') as f:\n",
    "        f.write(text_data)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit (NLTK) is a Python library for natural language processing (NLP) tasks. It provides a set of tools and resources for working with text data, including tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, and more.\n",
    "\n",
    "NLTK works by providing a set of functions and classes that allow users to preprocess and analyze text data. For example, to tokenize a piece of text into individual words, you can use the `word_tokenize` function from the `nltk.tokenize` module. Similarly, to perform part-of-speech tagging, you can use the `pos_tag` function from the `nltk.pos_tag` module.\n",
    "\n",
    "Under the hood, NLTK uses various algorithms and models to perform these tasks. For example, the `word_tokenize` function uses a regular expression to split the text into words, while the `pos_tag` function uses a statistical model to assign part-of-speech tags to each word based on its context.\n",
    "\n",
    "In addition to providing basic functions for text processing, NLTK also includes pre-trained models and datasets for various NLP tasks, as well as tools for training your own models. For example, the `nltk.sentiment.vader` module provides a pre-trained model for sentiment analysis using VADER, while the `nltk.corpus` module provides a collection of corpora and datasets for various languages and domains.\n",
    "\n",
    "Overall, NLTK provides a powerful and flexible toolkit for working with text data in Python, and its modular design and extensive documentation make it easy to customize and extend for specific tasks and applications.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based sentiment analysis tool that is integrated into the Natural Language Toolkit (NLTK). It uses a combination of lexicons, grammatical rules, and heuristics to identify the sentiment of a piece of text.\n",
    "\n",
    "At a high level, VADER works by first tokenizing a piece of text into words and then scoring each word based on its sentiment. The scoring is done using a lexicon of words that have been manually annotated with their sentiment polarity (positive, negative, or neutral) and intensity (strong or weak). The lexicon includes more than 7,500 lexical features, including common idioms and slang, that are not typically found in general-purpose dictionaries.\n",
    "\n",
    "Once the words have been scored, VADER applies a set of heuristics to adjust the sentiment scores based on various linguistic features, such as negation, emphasis, and conjunctions. For example, VADER can detect negation in a sentence (e.g., \"I don't like this product\") and adjust the sentiment score of the negated word accordingly.\n",
    "\n",
    "Finally, VADER combines the individual word scores to produce an overall sentiment score for the text. The score ranges from -1 (extremely negative) to +1 (extremely positive), with 0 indicating neutral sentiment.\n",
    "\n",
    "VADER is designed to work well on social media text, such as tweets and online reviews, which can be difficult to analyze using traditional sentiment analysis methods due to their informal language, sarcasm, and other nuances. However, it is important to note that VADER, like all sentiment analysis tools, has its limitations and may produce inaccurate results in certain contexts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TextBlob` library is used to calculate the subjectivity score of the text content. TextBlob is a Python library for processing textual data that provides an easy-to-use interface for common natural language processing (NLP) tasks, such as part-of-speech tagging, sentiment analysis, and noun phrase extraction.\n",
    "\n",
    "To calculate the subjectivity score, the code creates a `TextBlob` object using the `text_data` variable, which contains the text content of the article. The `sentiment` property of the `TextBlob` object is then used to calculate the sentiment polarity and subjectivity scores of the text. The `subjectivity` score represents the degree to which the text expresses opinions or feelings, as opposed to factual information. It ranges from 0 (completely objective) to 1 (completely subjective).\n",
    "\n",
    "Overall, the TextBlob library provides a convenient and intuitive way to perform common NLP tasks, and is a popular choice for beginners and small-scale projects. However, it may not be as accurate or robust as more advanced NLP tools, especially for complex or domain-specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article saved as: ai-in-healthcare-to-improve-patient-outcomes.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the URL of the web page to scrape\n",
    "url = 'https://insights.blackcoffer.com/ai-in-healthcare-to-improve-patient-outcomes/'\n",
    "\n",
    "# Make a GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the website using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all div elements with the class 'tdb-block-inner td-fix-index'\n",
    "example_divs = soup.find_all('div', {'class': 'tdb-block-inner td-fix-index'})\n",
    "\n",
    "# Extract text data from each div element and save it to a single file\n",
    "text_data = ''\n",
    "for div in example_divs:\n",
    "    text_data += div.get_text() + '\\n'\n",
    "\n",
    "# Set the file name to the URL ID and save the extracted article in a text file\n",
    "file_name = url.split('/')[-2] + '.txt'\n",
    "with open(file_name, 'w') as f:\n",
    "    f.write(text_data)\n",
    "\n",
    "# Print the file name to the console\n",
    "print('Article saved as:', file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 2074\n",
      "Sentiment: {'neg': 0.043, 'neu': 0.812, 'pos': 0.145, 'compound': 0.9997}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Tokenize the text content into words\n",
    "words = nltk.word_tokenize(text_data)\n",
    "\n",
    "# Calculate the word count\n",
    "word_count = len(words)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment = sid.polarity_scores(text_data)\n",
    "\n",
    "# Print the results to the console\n",
    "print('Word count:', word_count)\n",
    "print('Sentiment:', sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 2074\n",
      "Sentiment positive: 0.145\n",
      "Sentiment negative: 0.043\n",
      "Sentiment neutral: 0.812\n",
      "PCW: 18.23%\n",
      "Fog Index: 14.65\n",
      "Personal pronoun count: Counter({'it': 16, 'they': 4, 'our': 3, 'their': 3, 'its': 2, 'we': 1, 'us': 1, 'them': 1})\n",
      "Personal pronoun percentage: 1.49%\n",
      "syllable_per_word: 1.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\alok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\alok\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import textstat\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Tokenize the text content into words\n",
    "words = nltk.word_tokenize(text_data)\n",
    "\n",
    "# Calculate the word count\n",
    "word_count = len(words)\n",
    "\n",
    "# Perform sentiment analysis\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = sid.polarity_scores(text_data)\n",
    "\n",
    "# Extract the sentiment scores\n",
    "sentiment_positive = sentiment_scores['pos']\n",
    "sentiment_negative = sentiment_scores['neg']\n",
    "sentiment_neutral = sentiment_scores['neu']\n",
    "\n",
    "# Print the results to the console\n",
    "print('Word count:', word_count)\n",
    "print('Sentiment positive:', sentiment_positive)\n",
    "print('Sentiment negative:', sentiment_negative)\n",
    "print('Sentiment neutral:', sentiment_neutral)\n",
    "\n",
    "complex_word_count = sum(1 for word in words if textstat.syllable_count(word) >= 3)\n",
    "\n",
    "# Calculate the total number of words\n",
    "total_word_count = len(words)\n",
    "\n",
    "# Calculate the PCW as the percentage of complex words in the text\n",
    "pcw = complex_word_count / total_word_count * 100\n",
    "\n",
    "# Print the PCW to the console\n",
    "print(f\"PCW: {pcw:.2f}%\")\n",
    "\n",
    "# Calculate the Fog Index\n",
    "fog_index = textstat.gunning_fog(text_data)\n",
    "\n",
    "# Print the Fog Index to the console\n",
    "print(\"Fog Index:\", fog_index)\n",
    "\n",
    "\n",
    "\n",
    "# Define the set of personal pronouns\n",
    "personal_pronouns = set(['i', 'me', 'my', 'mine', 'you', 'your', 'yours', 'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs'])\n",
    "\n",
    "# Count the frequency of personal pronouns in the text\n",
    "personal_pronoun_count = Counter(word.lower() for word in words if word.lower() in personal_pronouns)\n",
    "\n",
    "# Calculate the total number of words\n",
    "total_word_count = len(words)\n",
    "\n",
    "# Calculate the percentage of personal pronouns in the text\n",
    "personal_pronoun_percentage = sum(personal_pronoun_count.values()) / total_word_count * 100\n",
    "\n",
    "# Print the results to the console\n",
    "print(f\"Personal pronoun count: {personal_pronoun_count}\")\n",
    "print(f\"Personal pronoun percentage: {personal_pronoun_percentage:.2f}%\")\n",
    "\n",
    "# Calculate the total number of syllables\n",
    "total_syllables = sum(textstat.syllable_count(word) for word in words)\n",
    "\n",
    "# Calculate the total number of words\n",
    "total_words = len(words)\n",
    "\n",
    "# Calculate the SPW as the average number of syllables per word\n",
    "syllable_per_word = total_syllables / total_words\n",
    "\n",
    "# Print the SPW to the console\n",
    "print(f\"syllable_per_word: {syllable_per_word:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity score: 0.4589452762999276\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Create a TextBlob object with the text content\n",
    "blob = TextBlob(text_data)\n",
    "\n",
    "# Calculate the subjectivity score\n",
    "subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "# Print the subjectivity score to the console\n",
    "print('Subjectivity score:', subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 26.935064935064936\n",
      "Average number of words per sentence: 26.94\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = nltk.sent_tokenize(text_data)\n",
    "\n",
    "# Calculate the total number of words and sentences\n",
    "num_words = 0\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "for sentence in sentences:\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Increment the word count\n",
    "    num_words += len(words)\n",
    "\n",
    "# Calculate the average sentence length\n",
    "avg_sentence_length = num_words / num_sentences\n",
    "\n",
    "# Print the result to the console\n",
    "print('Average sentence length:', avg_sentence_length)\n",
    "\n",
    "# Calculate the average number of words per sentence\n",
    "avg_word_count = total_word_count / num_sentences\n",
    "\n",
    "# Print the result to the console\n",
    "print(f\"Average number of words per sentence: {avg_word_count:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
